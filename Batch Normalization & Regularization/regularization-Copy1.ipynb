{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to create a data Trandform function here. there transform objects will be use fulll to apply on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.RandomRotation((-15.0, 15.0), fill=(1,)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
    "                                       ])\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Trainning and Testing data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here applying the transform function we hafve created.\n",
    "#What is target_transform , Need to understand\n",
    "\n",
    "train = datasets.MNIST('./', train = True,  transform = train_transforms,  target_transform = None,  download = True)\n",
    "test  = datasets.MNIST('./', train = False,  transform = test_transforms,   target_transform = None,  download = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuda Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print ('Is Cuda Available ?',cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "else:\n",
    "    torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the cuda availability we are defining the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_args = dict(shuffle = True,batch_size = 128,num_workers = 4, pin_memory = True) if cuda else dict(shuffle = True,batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,**dataloader_args)\n",
    "test_loader = torch.utils.data.DataLoader(test,**dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Statistics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = train.train_data\n",
    "train_data = train.transform(train_data.numpy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train.train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images,label = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0].numpy().squeeze(),cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "num_of_image = 60\n",
    "for index in range(1,num_of_image+1):\n",
    "    plt.subplot(6,10,index)\n",
    "    plt.axis ('off')\n",
    "    plt.imshow(images[index].numpy().squeeze(),cmap= 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# copied the GBN function from internet\n",
    "https://twitter.com/izariuo440/status/874058902760849410\n",
    "https://colab.research.google.com/github/pytorch/ignite/blob/master/examples/notebooks/Cifar10_Ax_hyperparam_tuning.ipynb#scrollTo=eSKwJ9TvkZUi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GhostBatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, eps=1e-05, momentum=0.1, weight=True, bias=True):\n",
    "        super(GhostBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum)\n",
    "        self.weight.data.fill_(1.0)\n",
    "        self.bias.data.fill_(0.0)\n",
    "        self.weight.requires_grad = weight\n",
    "        self.bias.requires_grad = bias        \n",
    "        self.num_splits = num_splits\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
    "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        if (self.training is True) and (mode is False):\n",
    "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "        return super(GhostBatchNorm, self).train(mode)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            return F.batch_norm(\n",
    "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W) \n",
    "        else:\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "self, , \n",
    "fmap_factor=64, conv_ksize=3, conv_pad=1, \n",
    "gbn_num_splits=512 // 32,                  \n",
    "classif_scale=0.0625):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GhostBatchNorm(num_classes, num_splits=512 // 32, weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBN_ind = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'nn.Dropout(self.dropout_value)' if GBN_ind == False else 'GhostBatchNorm(num_channels, num_splits=512 // 32, weight=False)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout_value = 0.05\n",
    "        # Input Block\n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(10),\n",
    "#             GhostBatchNorm(num_classes, num_splits= 512 // 32, weight=False),\n",
    "            nn.Dropout(self.dropout_value)  ,\n",
    "            nn.ReLU()\n",
    "        ) # input_size = 28 output_size = 26 receptive_field = 3\n",
    "\n",
    "        # CONVOLUTION BLOCK 1\n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.Dropout(self.dropout_value),\n",
    "            nn.ReLU(),\n",
    "        ) # input_size = 26 output_size = 24 receptive_field = 5\n",
    "        \n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.Dropout(self.dropout_value),\n",
    "            nn.ReLU()\n",
    "        ) # input_size = 24 output_size = 22 receptive_field = 7       \n",
    "        \n",
    "        self.convblock4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.Dropout(self.dropout_value),\n",
    "            nn.ReLU()\n",
    "        ) # input_size = 22 output_size = 20 receptive_field = 9        \n",
    "        \n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # input_size = 20 output_size = 10 receptive_field = 18\n",
    "\n",
    "        # CONVOLUTION BLOCK 2\n",
    "        self.convblock5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.Dropout(self.dropout_value),\n",
    "            nn.ReLU()\n",
    "        ) # input_size = 10 output_size = 8 receptive_field = 20\n",
    "        \n",
    "        self.convblock6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(self.dropout_value),\n",
    "            nn.ReLU()\n",
    "        ) # input_size = 8 output_size = 6 receptive_field = 22\n",
    "        \n",
    "        self.convblock7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(self.dropout_value),\n",
    "            nn.ReLU()\n",
    "        ) # input_size = 6 output_size = 4 receptive_field = 24\n",
    "        \n",
    "        self.gap = nn.AvgPool2d(kernel_size=(4,4)) \n",
    "        \n",
    "        self.convblock8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(10),\n",
    "        ) # input_size = 1 output_size = 1  receptive_field = 24\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.convblock4(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.convblock5(x)\n",
    "        x = self.convblock6(x)\n",
    "        x = self.convblock7(x)\n",
    "        x = self.gap(x)  \n",
    "        x = self.convblock8(x)\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.dropout_val = 0.10\n",
    "\n",
    "        # Convolution block-1\n",
    "        self.conv_blk1 = nn.Sequential(\n",
    "            # input layer\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0,  bias=False),   #Output: 8X26X26, Jin=1, GRF: 3X3\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(self.dropout_val),\n",
    "            \n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0,  bias=False),  #Output: 16X24X24, Jin=1, GRF: 5X5\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(self.dropout_val),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0,  bias=False), #Output: 16X22X22, Jin=1, GRF: 7X7\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(self.dropout_val)\n",
    "        )\n",
    "\n",
    "        # Transition Layer for Convolution block-1\n",
    "        self.conv_blk1_transition = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),                                                                     #Output: 16X11X11, Jin=1, GRF: 8X8\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0,  bias=False),  #Output: 8X11X11 , Jin=2, GRF: 8X8 (combining channels)\n",
    "        )\n",
    "\n",
    "        # Convolution block-2\n",
    "        self.conv_blk2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0,  bias=False),  #Output: 16X9X9,  Jin=2, GRF: 12X12\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(self.dropout_val),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0,  bias=False), #Output: 16X7X7, Jin=2, GRF: 16X16\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(self.dropout_val),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0,  bias=False), #Output: 16X5X5, Jin=2, GRF: 20X20\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(self.dropout_val)\n",
    "        )\n",
    "\n",
    "        # Output Block\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=5),                                                             #Output: 16X1X1, Jin=2, GRF: 28X28\n",
    "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0,  bias=False),  #Output: 10X1X1, Jin=2, GRF: 28X28 combining to 10 channels as we need 10 classes for predictions\n",
    "            # no ReLU for last covv layer\n",
    "            # No Batch Normalization\n",
    "            # No Dropout\n",
    "        ) # output_size = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_blk1(x) # convolution block-1\n",
    "        x = self.conv_blk1_transition(x)\n",
    "        x = self.conv_blk2(x) # convolution block-2\n",
    "        # output \n",
    "        x = self.output_block(x) # \n",
    "        # flatten the tensor so it can be passed to the dense layer afterward\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "model = Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L1_Loss_calc(model, factor=0.0005):\n",
    "    l1_crit = nn.L1Loss(size_average=False)\n",
    "    reg_loss = 0\n",
    "    for param in model.parameters():\n",
    "        #zero_vector = torch.rand_like(param)*0\n",
    "        zero_vector = torch.zeros_like(param)\n",
    "        reg_loss += l1_crit(param,zero_vector)\n",
    "    return factor * reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1_loss check "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = L1_Loss_calc(model,factor=0.0005)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp.item()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, l1_check =False):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # get samples\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.nll_loss(y_pred, target)\n",
    "        #adding L1 loss function call \n",
    "\n",
    "        if( l1_check == True ):\n",
    "            regloss = L1_Loss_calc(model, 0.0005)\n",
    "            regloss /= len(data) # by batch size\n",
    "            loss += regloss\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update pbar-tqdm\n",
    "\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "\n",
    "#         pbar.set_description(desc= f'Loss={loss.item():0.6f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "        pbar.set_description(desc= f'Loss={train_loss/(batch_idx+1):0.6f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    acc = np.round(100. * correct/len(train_loader.dataset),2) #processed # \n",
    "    return acc, np.round(train_loss,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model, device, test_loader,l1_check = False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if( l1_check == True ):\n",
    "            regloss = L1_Loss_calc(model, 0.0005)\n",
    "            regloss /= len(data) # by batch size\n",
    "            test_loss += regloss\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100 * correct / len(test_loader.dataset)))\n",
    "    acc = 100 * correct /len(test_loader.dataset)\n",
    "    acc = np.round(acc,2)\n",
    "    \n",
    "    return acc, test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp Store"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "model =  Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, m`omentum=0.9)\n",
    "\n",
    "for epoch in range(1,EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    acc,loss =  train(model, device, train_loader, optimizer, epoch,l1_check= True)\n",
    "    train_acc.append(acc)\n",
    "    train_losses.append(loss)\n",
    "\n",
    "test_losses = []\n",
    "test_acc = []\n",
    "\n",
    "model =  Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "for epoch in range(1,EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    acc,loss =  test(model, device, test_loader,l1_check= True)\n",
    "    test_acc.append(acc)\n",
    "    test_losses.append(loss)\n",
    "\n",
    "model =  Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "EPOCHS = 6\n",
    "for epoch in range(1,EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    acc,loss =  train(model, device, train_loader, optimizer, epoch,l1_check= True)\n",
    "    train_acc.append(acc)\n",
    "    train_losses.append(loss)\n",
    "    acc,loss =  test(model, device, test_loader,l1_check= True)\n",
    "    test_acc.append(acc)\n",
    "    test_losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets add L2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(typ, model, device,train_loader, test_loader,l1_check = False, l2_val = 0, EPOCHS = 3):\n",
    "    \n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "    \n",
    "    print ('Model with ', typ)\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        print(\"EPOCH:\", epoch)\n",
    "        acc,loss =  train(model, device, train_loader, optimizer, epoch,l1_check= True)\n",
    "        train_acc.append(acc)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        acc,loss =  test(model, device, test_loader,l1_check= True)\n",
    "        test_acc.append(acc)\n",
    "        test_losses.append(loss)\n",
    "        \n",
    "    return train_acc,train_losses,test_acc,test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending task  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tmp store "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with L1 + BN\n",
    "with L2 + BN\n",
    "with L1 and L2 with BN\n",
    "with GBN\n",
    "with L1 and L2 with GBN\n",
    "\n",
    "models = ['L1+BN','L2+BN','L1 & L2 + BN','GBN','L1 & L2 + GBN']\n",
    "\n",
    "for name  in models :\n",
    "    model =  Net().to(device)\n",
    "    train_acc,train_losses,test_acc,test_losses = model_building(name,model, device,train_loader, test_loader,l1_check = False,l2_val = 0,EPOCHS = 10)   \n",
    "    \n",
    "\n",
    "name = [L1_Loss_calc for name in models]\n",
    "\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model =  Net().to(device)\n",
    "train_acc,train_losses,test_acc,test_losses = model_building(model, device,train_loader, test_loader,l1_check = False,l2_val = 0,EPOCHS = 10)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_classification(typ, model, device, testloader, num_of_images = 25, save_filename=\"misclassified\"):\n",
    "    model.eval()\n",
    "    misclassified_cnt = 0\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)     # get the index of the max log-probability\n",
    "        pred_marker = pred.eq(target.view_as(pred))   \n",
    "        wrong_idx = (pred_marker == False).nonzero()  # get indices for wrong predictions\n",
    "        print ('Missclassification on ', typ)\n",
    "        for idx in wrong_idx:\n",
    "            index = idx[0].item()\n",
    "            title = \"Actul:{}, Pred:{}\".format(target[index].item(), pred[index][0].item())\n",
    "            #print(title)\n",
    "            ax = fig.add_subplot(5, 5, misclassified_cnt+1, xticks=[], yticks=[]) #ax.axis('off')\n",
    "            ax.set_title(title)\n",
    "            plt.imshow(data[index].cpu().numpy().squeeze(), cmap='gray_r')\n",
    "            misclassified_cnt += 1\n",
    "            if(misclassified_cnt==num_of_images):\n",
    "                break\n",
    "        if(misclassified_cnt==num_of_images):\n",
    "            break\n",
    "    \n",
    "    fig.savefig(\"{}.png\".format(save_filename)) \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(typ, train_acc,train_losses,test_acc,test_losses):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(20,5))\n",
    "    print ('Accuracy model on  ', typ)\n",
    "    plt.autoscale()\n",
    "    axs[0].plot(train_acc,color = 'red')\n",
    "    axs[0].plot(test_acc,color = 'green')\n",
    "    title = 'Training/testing accuracy'\n",
    "    axs[0].set_title(title)\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train','test'], loc='best')\n",
    "       \n",
    "    axs[1].plot(train_losses,color = 'red')\n",
    "    axs[1].plot(test_losses,color = 'green')\n",
    "    title = 'Training/Testing Loss'\n",
    "    axs[1].set_title(title)\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train','test'], loc='best')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_acc_loss(train_acc,train_losses,test_acc,test_losses)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model =  Net().to(device)\n",
    "train_acc,train_losses,test_acc,test_losses = model_building(model, device,train_loader, test_loader,l1_check = False,l2_val = 0,EPOCHS = 25)\n",
    "plot_acc_loss(train_acc,train_losses,test_acc,test_losses)\n",
    "miss_classification(model, device, testloader = test_loader, num_of_images = 25, save_filename=\"misclassified\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "models = ['L1+BN','L2+BN','L1 & L2 + BN','GBN','L1 & L2 + GBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelcalling(models):\n",
    "\n",
    "    for typ in models:\n",
    "        if typ == 'L1+BN':\n",
    "            l1_check = True\n",
    "            l2_val = 0\n",
    "        elif typ == 'L2+BN':\n",
    "            l1_check = False\n",
    "            l2_val = 0.03\n",
    "        elif typ == 'L1 & L2 + BN':\n",
    "            l1_check = True\n",
    "            l2_val = 0.03\n",
    "        elif typ == 'GBN':\n",
    "            print ('thi9nk')\n",
    "        elif typ == 'L1 & L2 + GBN':\n",
    "            print ('think')\n",
    "            \n",
    "        model =  Net().to(device)\n",
    "        train_acc,train_losses,test_acc,test_losses = model_building(typ, model, device,train_loader, test_loader,l1_check = False,l2_val = 0,EPOCHS = 10)\n",
    "        plot_acc_loss(typ, train_acc,train_losses,test_acc,test_losses)\n",
    "        miss_classification(typ, model, device, testloader = test_loader, num_of_images = 25, save_filename=\"misclassified\")\n",
    "        \n",
    "        modelparams = {'train_acc':train_acc,'test_acc':test_acc,'train_losses':train_losses,'test_losses':test_losses.type()}\n",
    "        file = pd.DataFrame(modelparams)\n",
    "        file.to_csv(typ + 'param'+'.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "models = ['L1+BN','L2+BN','L1 & L2 + BN','GBN','L1 & L2 + GBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['L1+BN','L2+BN']\n",
    "modelcalling(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
